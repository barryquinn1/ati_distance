{
  "hash": "c83b4fce336e22c95bc98828c5ee2906",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Distance metrics\"\nsubtitle: \"How information theory can help solve real-world financial problems\"\nbibliography: [\"refs.bib\"]\ncss: \"mycssblend.css\"\nembed-resources: true\ntitle-slide-attributes:\n  data-background-image: img/title-slide.png\n  data-background-size: cover\n  data-background-opacity: \"0.5\"\nformat: \n  revealjs:\n    slideNumber: c/t\n    scrollable: true\nlogo: img/qbslogo.png\neditor: visual\nfooter: \"AI & Trading\"\nexecute:\n  warning: false\n  error: false\n  echo: true\n---\n\n\n## Outline {.saltinline}\n\n-   Distance metrics\n    -   Correlation based distance metrics\n-   Shannon's entropy\n    -   Marginal, conditional, joint,mutual information and variation of information\n-   Experimental evidence\n-   Financial problem investigation\n\n## Distance Metrics {.saltinline .small}\n\n-   Many problems in finance require the clustering of variables or observations:\n\n1.  Factor investing, relative value analysis (e.g., forming quality minus junk portfolios)\n2.  Risk management, portfolio construction (e.g., deriving the efficient frontier)\n3.  Dimensionality reduction (e.g., decomposing bond return drivers)\n4.  Modelling of multicollinear systems (e.g., computing p-values)\n\n## Distance Metrics {.saltinline .small}\n\nSo Far We Have Studied...\n\n‚Ä¢ The important numerical properties of the empirical correlation (and by extension, covariance) matrix.\n\n**Critical Limitations of Correlation**\n\n‚Ä¢ Despite its virtues, correlation suffers from several critical limitations as a measure of codependency.\n\n**Overcoming These Limitations**\n\n‚Ä¢ In this lecture, we will overcome these limitations by reviewing information theory concepts that underlie many modern marvels.\n\n**Information Theory Concepts in modern life**\n\n‚Ä¢ Internet, mobile phones, file compression, video streaming, and encryption.\n\n**Why We Looked Beyond Correlation**\n\n‚Ä¢ None of these inventions would have been possible if researchers had not looked beyond correlations to understand codependency.\n\n## Claude Shannon's Entropy {.small}\n\nüîç **Information Theory Applications in Finance**\n\n‚Ä¢ As it turns out, information theory in general, and the concept of Shannon's entropy in particular, also have useful applications in finance. üí°\n\n‚Ä¢ The key idea behind entropy is to quantify the amount of uncertainty associated with a random variable. üîç\n\n‚Ä¢ Information theory is also essential to ML, because the primary goal of many ML algorithms is to reduce the amount of uncertainty involved in the solution to a problem. üíØ\n\nüëâ We will see how Shannon's entropy, a key concept in information theory, can help solve real-world financial problems! üìà\n\n## Desirably properties of a distance metric {.small}\n\n-   In mathematics, a distance function or metric is a generalisation of the concept of physical distance. A metric is a function that defines a distance between each pair of elements of a set.\n\n1.  Non-negativity: $d(x, y) \\geq 0$\n2.  Identity of indiscernibles: $d(x, y) = 0$ if and only if $x = y$\n3.  Symmetry: $d(x, y) = d(y, x)$\n4.  Triangle inequality: $d(x, y) + d(y, z) \\geq d(x, z)$\n\n## Correlation is not a metric {.small}\n\nConsider Two Random Vectors...\n\n‚Ä¢ Let X and Y be two random vectors of size T, and a correlation estimate œÅ(X,Y), with the only requirement that œÉ(X,Y) = œÅ(X,Y)œÉ(X)œÉ(Y).\n\n‚Ä¢ œÉ(X,Y) is the covariance between the two vectors.\n\n‚Ä¢ œÉ(X) and œÉ(Y) are the standard deviations of X and Y, respectively.\n\nPearson's Correlation...\n\n‚Ä¢ Pearson's correlation is one of several correlation estimates that satisfy these requirements.\n\n## Correlation is not a metric\n\n-   Pearson's correlation is a measure of the linear relationship between two variables.\n-   It is a number between -1 and 1.\n-   It does not have the properties of a distance metric.\n-   Specifically, it does not satisfy the triangle inequality or non-negativity.\n\n## A correlation-based distance metric\n\n-   The correlation-based distance metric is defined as: $$d(x, y) = \\sqrt{2(1 - \\rho(x, y))}$$ where $\\rho(x, y)$ is the correlation between $x$ and $y$.\n\n-   This metric does satisfy the properties of a distance metric.\n\n## Proof {visibility=\"hidden\"}\n\nüìù Euclidean Distance Definition\n\n1.  The Euclidean distance between two vectors is defined as:\n\n$$d\\left(X,Y\\right)=\\sqrt{\\sum_{t=1}^{T}\\left(X_t-Y_t\\right)^2}$$\n\nüí° Z-Standardization\n\n2.  Z-standardise the vectors so that $x = \\left(X - \\bar{X}\\right)/\\sigma\\left(X\\right)$ and $y = \\left(Y - \\bar{Y}\\right)/\\sigma\\left(Y\\right)$, where $\\bar{.}$ is the mean value.\n\nüìà Correlation Derivation\n\n3.  We derive the Euclidean distance $d\\left(x,y\\right)$:\n\n$$d\\left(x,y\\right) = \\sqrt{\\sum_{t=1}^{T}\\left(x_t-y_t\\right)^2} = \n\\sqrt{T+T=2T\\sigma\\left(x,y\\right)} = \\sqrt{4T}d+p(X,Y)$$\n\nüîç Inheritance of True-Metric Properties\n\n4.  This implies that $d_p(X,Y)$ is a linear multiple of the Euclidean distance between the vectors $\\left\\{X,Y \\right\\}$ after z-standardization $d(x,y)$, thus inheriting the true-metric properties of the Euclidean distance.\n\n## Properties of Metric $d_p(x,y)$ {.small}\n\nüìù Normalisation\n\n1.  It is normalized, $d_p(X,Y) \\in[0,1]$, because $\\rho(X,Y) \\in[-1,1]$.\n\nüîç Non-negativity Property\n\n2.  It deems more distant two random variables with negative correlation than two random variables with positive correlation, regardless of their absolute value.\n\nüí° Financial Applications\n\n> This property is very useful in finance, for example, we may wish to build a long-only portfolio where holdings in negative-correlated securities can only offset risk and therefore should be treated as different for diversification purposes.\n\n## An Alternative Correlation-Based Distance Metric{.small}\n\n::: columns\n::: column\nüìù Aim\n\n-   In a long-short portfolio, we often prefer to consider highly negatively correlated securities as similar, because the position sign can override the sign of the correlation. For that case, we can define an alternative normalized correlation-based distance metric:\n\n$$d_{|p|} = \\sqrt{1 - | \\rho(X,Y) |}$$\n\nüí° Metric Definition\n\n-   This metric is defined as the square root of the absolute value of the correlation between the two vectors minus 1.\n:::\n\n::: column\nüîç Advantages\n\n::: saltinline\n-   It treats highly negatively correlated securities as similar, which can be useful in a long-short portfolio.\n-   It is based on the absolute value of the correlation, which can be more intuitive than the traditional Euclidean distance metric.\n-   It is normalized, meaning that it has a minimum value of 0 and a maximum value of 1, which can make it easier to interpret.\n:::\n:::\n:::\n\n## Entropy: What is it?\n\n-   Entropy is a measure of uncertainty or randomness in a random variable.\n-   It is a measure of the average amount of information produced by a stochastic source of data.\n-   It is a measure of the unpredictability of information content.\n-   It is a measure of the disorder in a system.\n-   In finance, entropy can be used to measure the uncertainty in the returns of a stock or a portfolio.\n\n## Entropy in Quantitative Finance {.small}\n\n-   The concept of correlation has three important limitations:\n\n1.  Firstly, it quantifies the linear codependency between two random variables, but neglects nonlinear relationships.\n2.  Secondly, correlation is highly influenced by outliers.\n3.  Finally, its application beyond the multivariate normal case is questionable.\n    \n\n## Entropy and the challenge of measuring accuracy\n\n-   Information theory provides a natural measurement scale for distance between two probability distributions\n-   Need to establish deviance as an approximation of relative distance from perfect accuracy\n-   Entropy is a measure of the uncertainty of a random variable\n\n## Desirable properties of good uncertainty measure\n\n-   Measure should be continuous\n-   Should increase as the number of possible events increases\n-   Should be additive\n\n## Information entropy\n\n-   Definition: -E(log(p_i))= -‚àë(p_i) log(p_i)\n-   Intuitive explanation: uncertainty contained in a probability distribution is the average log-probability of an event\n-   Viewed as the expected value of these surprises\n-   Accepted as a useful measure of uncertainty not because of premises that lead to it, but rather because it has turned out to be so useful and productive\n\n## Practical applications of entropy\n\n-   Lets calculate the information entropy for the weather tomorrow\n-   Suppose the true probabilities of rain and shine are $p_1=0.3$ and $p_2=0.7$, respectively. Then: $$H(p)=-(p_1 log(p_1)+p_2 log(p_2)) \\approx 0.61$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- c(0.3,0.7)\n-sum(p*log(p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6108643\n```\n\n\n:::\n:::\n\n\n## Practical applications of entropy\n\n-   Suppose instead we live in Abu Dhabi\n-   Then the probabilities of rain and shine might be more like $p_1=0.01$ and $p_2=0.99$\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- c(0.01,0.99)\n-sum(p*log(p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05600153\n```\n\n\n:::\n:::\n\n\n## Practical applications of entropy\n\n-   now the entropy would be approximately 0.06\n-   Why has the uncertainty decreased?\n\n. . .\n\n-   Because in Abu Dhabi it hardly ever rains.\n-   Therefore there‚Äôs much less uncertainty about any given day, compared to a place in which it rains 30% of the time.\n-   It‚Äôs in this way that information entropy measures the uncertainty inherent in a distribution of events.\n\n## Entropy and forecasting complexity {.small}\n\n-   Similarly, if we add another kind of event to the distribution,forecasting into winter, so also predicting snow, entropy tends to increase, due the added dimensionality of the prediction problem.\n-   Similarly, if we add another kind of event to the distribution, <span class=\"blue\">forecasting into winter, so also predicting snow,</span> entropy tends to increase, due the added dimensionality of the prediction problem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np<-c(0.7,0.15,0.15)\n-sum(p*log(p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8188085\n```\n\n\n:::\n:::\n\n\n-   Then entropy is about 0.82\n\n## Entropy Definition\n\n-   Recall the definition of entropy is $-\\sum_{x \\in S} p(x) log(p(x))$, where $S$ is the sample space of the random variable.\n-   This formula assumes that the events are mutually exclusive and exhaustive, which may not always be the case in real-world scenarios.\n\n## Logarithms\n\n-   The logarithms used in entropy calculations can be either natural logs (base e) or binary logs (base 2).\n-   However, it's important to note that different bases can lead to different interpretations of entropy.\n-   When comparing entropies across different random variables, it's essential to use the same base.\n\n## Zero Probability Events\n\n-   $log(0) = \\infty$, which means that events with probability 0 contribute nothing to the entropy.\n-   To avoid any inconsistencies, we set $log(0) = 0$ when computing entropy.\n\n## Joint Entropy\n\n-   The joint entropy of two random variables X and Y is $-\\sum_{x,y \\in S_X,S_Y} p(x,y) log(p(x,y))$, where $S_X$ and $S_Y$ are the sample spaces of X and Y, respectively.\n-   This formula assumes that the events are mutually exclusive and exhaustive.\n\n## Properties of Joint Entropy {.small}\n\n-   Non-negativity: $H(X,Y) \\ge 0$.\n-   Symmetry: $H(X,Y) = H(Y,X)$.\n-   Greater than individual entropies: $H(X,Y) \\ge max(H(X),H(Y))$.\n-   Less than or equal to the sum of individual entropies: $H(X,Y) \\le H(X) + H(Y)$.\n\n## Calculating Joint Entropy in Finance with R\n\n-   Joint entropy quantifies the uncertainty in a joint distribution of two random variables.\n-   Useful for analyzing the combined behavior of financial market variables.\n-   We'll demonstrate using simulated asset returns.\n\n## Setup in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ensure the infotheo package is installed and loaded\nif (!requireNamespace(\"infotheo\", quietly = TRUE)) install.packages(\"infotheo\")\nlibrary(infotheo)\n```\n:::\n\n\n## Simulating Asset Returns\n\n-   Simulate daily returns for two hypothetical financial assets.\n-   Assume normally distributed returns for simplicity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) # Ensuring reproducibility\nreturns_asset1 = rnorm(100, mean = 0.001, sd = 0.01) # Asset 1 returns\nreturns_asset2 = rnorm(100, mean = 0.001, sd = 0.02) # Asset 2 returns\n```\n:::\n\n\n## Discretising Returns\n\n-   Discretisation is necessary for calculating entropy.\n-   The choice of bins is crucial and can affect the outcome.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbins = seq(from = min(c(returns_asset1, returns_asset2)), to = max(c(returns_asset1, returns_asset2)), length.out = 10)\ndiscrete_returns1 = cut(returns_asset1, breaks = bins, labels = FALSE)\ndiscrete_returns2 = cut(returns_asset2, breaks = bins, labels = FALSE)\n```\n:::\n\n\n## Calculating Joint Entropy {.small}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate mutual information (and hence joint entropy)\njoint_entropy = mutinformation(discrete_returns1, discrete_returns2, method = \"emp\")\nprint(paste(\"Joint entropy (natural log base):\", joint_entropy))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Joint entropy (natural log base): 0.194283857148803\"\n```\n\n\n:::\n:::\n\n\n-   If you specifically need the entropy value in bits, you can manually convert the output from natural logarithm to log base 2 by dividing the result by `log(2)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert the result to bits\njoint_entropy_bits = joint_entropy / log(2)\nprint(paste(\"Joint entropy (in bits):\", joint_entropy_bits))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Joint entropy (in bits): 0.280292357233359\"\n```\n\n\n:::\n:::\n\n\n-   Joint entropy provides insights into the information contained within the combined behavior of two financial assets.\n-   This example used simulated data and basic discretization for illustration.\n\n## Economic Interpretation of Joint Entropy in Finance {.small}\n\n-   **Complexity of Asset Relationships:** A joint entropy of $\\approx 0.28$ bits suggests a moderate level of unpredictability and complexity in the relationship between the two assets' returns. This indicates some degree of informational efficiency but also room for diversification.\n-   **Market Behaviour:** The specific value reflects the combined information content and interdependency of the assets under study, implying that while they share some information, each still possesses unique characteristics.\n\n## Strategic Implications {.small}\n\n-   **Diversification Benefits:** The calculated joint entropy value points towards potential diversification benefits. It suggests that the assets' price movements are not perfectly correlated, which can help in reducing overall portfolio volatility.\n-   **Risk Management and Investment Decisions:** This joint entropy value should inform risk management strategies by highlighting the importance of considering the complexity and predictability of asset returns. Investors should use this alongside other analyses for a comprehensive investment strategy.\n\n## Understanding Entropy in Finance {.small}\n\n-   **Conditional Entropy:**\n    -   Formula: (H(X\\|Y) = H(X,Y) - H(Y))\n    -   Represents the expected uncertainty in one variable ((X)) given the knowledge of another ((Y)). Essential for modeling predictability in financial markets.\n-   **Maximum Entropy (MaxEnt):**\n    -   Approach: Maximize (H(X)) subject to known constraints.\n    -   Identifies the least presumptive probability distributions based on known information, central to Bayesian inference and machine learning.\n\n## Understanding Entropy in Finance {.small}\n#### Divergence and Its Financial Significance\n\n-   **Kullback-Leibler (KL) Divergence:**\n    -   Formula: (D\\_{KL}(p, q) = \\sum\\_i p_i \\log\\left(\\frac{p_i}{q_i}\\right))\n    -   Measures the extra uncertainty induced when using distribution (q) to approximate (p), crucial for assessing model accuracy.\n-   **Cross-Entropy:**\n    -   Formula: (H_c(p, q) = -\\sum\\_{x\\in S_X}p(x) \\log(q(x)))\n    -   Highlights the discrepancy between the true distribution ((p)) and the predicted one ((q)), essential for evaluating classification models in finance.\n## Understanding Entropy in Finance {.small}\n#### Mutual Information and Strategic Insights \n\n-   **Mutual Information:**\n    -   Formula: (I(X;Y) = H(X) - H(X\\|Y) = H(X) + H(Y) - H(X,Y))\n    -   Quantifies the informational gain in variable (X) from knowing (Y), demonstrating the value of shared information between financial variables for strategic decision-making.\n\n## Understanding Entropy in Finance {.small}\n#### Implications for Financial Machine Learning\n\n-   **Model Selection and Evaluation:** Theoretical concepts like divergence and cross-entropy help in selecting and evaluating models based on their fidelity to true market behaviors and predictive capabilities.\n-   **Strategic Decision-Making:** Insights from mutual information and maximum entropy support financial practitioners in crafting robust investment strategies, managing risk, and optimizing portfolios with a principled understanding of market uncertainty.\n\n## Practical financial example\n\n-   First, we install and load the `quantmod` package, then download stock data for AAPL and MSFT.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"quantmod\")\nlibrary(quantmod)\n\n# Specify the stocks and time period\nstocks <- c(\"AAPL\", \"MSFT\")\nstart_date <- as.Date(\"2020-01-01\")\nend_date <- as.Date(\"2020-12-31\")\n\n# Download stock data\ngetSymbols(stocks, from=start_date, to=end_date, auto.assign = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"AAPL\" \"MSFT\"\n```\n\n\n:::\n:::\n\n\n## Calculate Daily Returns\n\nNext, we calculate the daily returns for each stock.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreturns_aapl <- dailyReturn(AAPL)\nreturns_msft <- dailyReturn(MSFT)\n#add both plots together\npar(mfrow=c(2,1))\nplot(returns_aapl, main = \"AAPL Daily Returns\")\nplot(returns_msft, main = \"MSFT Daily Returns\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(paste(\"Linear correlation (in bits):\", cor(returns_aapl,returns_msft)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Linear correlation (in bits): 0.839278219328718\"\n```\n\n\n:::\n:::\n\n\n## Discretize Returns and Calculate Information Measures\n\nTo apply the `infotheo` functions, we need to discretize the returns. Then, we calculate the conditional entropy and mutual information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(infotheo)\n\n# Discretise returns\ndiscretized_aapl <- discretize(as.numeric(returns_aapl))\ndiscretized_msft <- discretize(as.numeric(returns_msft))\n\n# Mutual Information between AAPL and MSFT returns\nmutual_info <- mutinformation(discretized_aapl, discretized_msft, method = \"emp\")\nprint(paste(\"Mutual Information between AAPL and MSFT:\", mutual_info))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Mutual Information between AAPL and MSFT: 0.516904960511519\"\n```\n\n\n:::\n:::\n\n\n## Variation of Information\n\n-   The Variation of Information (VI) measure, also known as shared information distance, is a metric used in information theory to quantify the distance or dissimilarity between two random variables or between two clusterings.\n\n-It combines the concepts of mutual information and entropy to measure how much two variables or clusterings differ in terms of their information content.\n\n## Overview of Variation of Information (VI) {.small}\n\n-   **Definition:** A metric that quantifies the dissimilarity between two random variables or clusterings by combining mutual information (I) and entropy (H).\n-   **Formula:** (VI(X, Y) = H(X) + H(Y) - 2I(X; Y))\n-   **Interpretation:** Measures the total amount of uncertainty or information that is unique to each of the variables (X) and (Y), not shared between them.\n\n## Significance in Finance\n\n-   **Cluster Analysis:** Used to evaluate the similarity between different financial market segmentations or clustering of assets based on returns, volatility, or other characteristics.\n-   **Time Series Analysis:** Helps in comparing the informational content of different financial time series, such as stock prices or economic indicators.\n-   **Risk Management:** Assists in identifying diversification opportunities by quantifying the dissimilarity in information content between asset returns.\n\n## Applications\n\n-   **Portfolio Optimization:** Analyzing the variation of information between asset returns to optimize portfolio diversification and risk-adjusted returns.\n-   **Market Segmentation:** Evaluating the effectiveness of market segmentation strategies by measuring the dissimilarity in information content across segments.\n-   **Economic Analysis:** Comparing economic indicators to understand the unique and shared information contributing to economic forecasts.\n\n## Benefits and Challenges {.small}\n\n::: columns\n::: column\n#### Benefits\n\n-   **Quantitative Measure:** Provides a clear, quantitative metric to assess dissimilarity in information content.\n-   **Insightful:** Offers deeper insights into the structure and dynamics of financial markets.\n-   **Decision Support:** Aids in making informed decisions by highlighting differences in informational content between variables.\n:::\n\n::: column\n#### Challenges\n\n-   **Data Intensive:** Requires comprehensive data for accurate calculation and interpretation.\n\n-   **Complexity:** Understanding and applying VI can be complex, requiring a solid foundation in information theory.\n\n-  **Interpretation:** Interpreting the results of VI analysis in the context of financial decision-making can be challenging.\n:::\n:::\n\n## TL;DR {.small}\n\n| Metric                                | Definition                                                                                                                  | Relationship                                                                       | Applications in Finance                                                                           | Metric Properties                                                                                                                                     |\n|---------------|---------------|---------------|---------------|---------------|\n| Entropy (H)                           | Measures the uncertainty or unpredictability of a variable's outcome.                                                       | Fundamental to all other metrics as a measure of uncertainty.                      | Quantifying the unpredictability of financial variables such as asset returns, market volatility. | **No**, entropy is not a distance measure and does not satisfy metric properties.                                                                     |\n| Mutual Information (I)                | Measures the amount of information shared between two variables; how much knowing one reduces uncertainty about the other.  | Relates to entropy by quantifying the reduction in uncertainty.                    | Analyzing dependencies between financial variables, identifying market trends.                    | **No**, mutual information is not a distance measure; it lacks symmetry and the triangle inequality for distances.                                    |\n| Conditional Entropy (H(X\\|Y))         | Quantifies the remaining uncertainty in one variable when the state of another is known.                                    | Derived from entropy, showing the reduction in uncertainty given another variable. | Assessing predictability of financial variables given others.                                     | **No**, conditional entropy is directional and does not satisfy symmetry or the triangle inequality.                                                  |\n| Cross-Entropy (Hc)                    | Measures the expected number of bits needed to identify an event from a set, based on a different probability distribution. | Incorporates KL divergence when comparing two distributions.                       | Evaluating performance of predictive models in finance.                                           | **No**, cross-entropy is not symmetric and does not satisfy the triangle inequality.                                                                  |\n| Kullback-Leibler Divergence (D\\_{KL}) | Quantifies the difference between two probability distributions.                                                            | Measures the information gain from one distribution to another.                    | Measuring the discrepancy between model predictions and actual data.                              | **No**, KL divergence is not symmetric and does not satisfy the triangle inequality.                                                                  |\n| Variation of Information (VI)         | A metric to quantify the dissimilarity between two random variables or clusterings.                                         | Combines entropy and mutual information to measure total unique information.       | Comparing financial market segmentations, understanding diversification.                          | **Yes**, variation of information is a true metric as it satisfies non-negativity, identity of indiscernibles, symmetry, and the triangle inequality. |\n\n## Experimental evidence {.small}\n\n- To calculate the Variation of Information (VI) in R and compare it with linear correlation, especially emphasizing the differences in detecting non-linear relationships, we'll first simulate data to create an exaggerated non-linear relationship. \n- Then, we'll calculate both the Pearson correlation coefficient (a measure of linear correlation) and the Variation of Information.\n\n## Experimental evidence {.small}\n\nFirst, let's simulate the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) # Ensure reproducibility\nx <- seq(-10, 10, by = 0.1)\ny <- sin(x)^2 + rnorm(length(x), sd = 0.1) # Non-linear relationship with added noise\nplot(x, y, main = \"Simulated Non-linear Relationship\", xlab = \"X\", ylab = \"Y\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n### Calculating Pearson Correlation in R\n\nThe Pearson correlation can be directly calculated using the `cor()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_correlation <- cor(x, y)\nprint(paste(\"Linear Correlation:\", linear_correlation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Linear Correlation: -0.00696311389621409\"\n```\n\n\n:::\n:::\n\n\n## Calculating Variation of Information in R {.small}\n\n- Calculating the Variation of Information (VI) in R for continuous variables directly is not as straightforward because it typically requires the variables to be discretized or involves estimating their joint and individual entropies. \n\n- Since there's no built-in function for VI in base R or common packages, we'll need to implement it. This involves discretizing the data, calculating the entropies, and then using the VI formula.\n\n\n## Calculating Variation of Information in R {.small}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(entropy) # Load the infotheo package for entropy and mutual information calculations\nx_disc <- discretize(x,numBins = 20) # Discretize x into 10 bins\ny_disc <- discretize(y, numBins=20) # Discretize y into 10 bins\n\n# Convert discretized data to numeric indices\nx_bins <- as.numeric(factor(x_disc))\ny_bins <- as.numeric(factor(y_disc))\n\n# Create contingency table for joint distribution\njoint_distribution <- table(x_bins, y_bins)\n\n# Calculate individual entropies\nH_x <- entropy(table(x_bins))\nH_y <- entropy(table(y_bins))\n\n# Calculate joint entropy\nH_xy <- entropy(joint_distribution)\n\n# Calculate mutual information\nI_xy <- H_x + H_y - H_xy\n\n# Calculate Variation of Information\nVI <- H_x + H_y - 2 * I_xy\nprint(paste(\"Variation of Information:\", VI))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Variation of Information: 2.23006570879938\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# For the Pearson correlation, no change is needed\nlinear_correlation <- cor(x, y)\nprint(paste(\"Linear Correlation:\", linear_correlation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Linear Correlation: -0.00696311389621409\"\n```\n\n\n:::\n:::\n\n\n## Normalized Variation of Information {.small}\n\n-   One common approach is to normalize VI by the joint entropy H(X,Y)H(X,Y) or by the sum of the individual entropies H(X)+H(Y)H(X)+H(Y), depending on the context and what aspect of the data you're most interested in:\n\n-   Normalized VI=VI(X,Y)H(X)+H(Y)Normalized VI=H(X)+H(Y)VI(X,Y)‚Äã\n\n-   This normalization ensures that the VI lies between 0 and 1, where 0 indicates that the two variables share all their information (identical distributions) and 1 indicates that the two variables share no information.\n\n## Normalized Variation of Information in R {.small}\n\n::: columns\n:::\n-   Let's continue from the previous example, assuming `H_x`, `H_y`, and `I_xy` (mutual information) have already been calculated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assuming H_x, H_y, and I_xy have already been calculated\n\n# Calculate Variation of Information (VI) again for clarity\nVI <- H_x + H_y - 2 * I_xy\n\n# Normalize VI to range between 0 and 1\n# Normalized VI is VI divided by the sum of the individual entropies (max possible VI)\nNormalized_VI <- VI / (H_x + H_y)\n\nprint(paste(\"Variation of Information:\", VI))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Variation of Information: 2.23006570879938\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Normalized Variation of Information:\", Normalized_VI))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Normalized Variation of Information: 0.896160537565619\"\n```\n\n\n:::\n:::\n\n\n::: \n::: column\n#### Interpretation \n- **Variation of Information (VI):** Provides a measure of the total unique and shared information between two variables. A lower value indicates more shared information, while a higher value indicates less shared information and more uniqueness. \n\n- **Normalized Variation of Information:** Adjusts VI into a 0 to 1 scale, where 0 indicates perfect agreement (identical information content) and 1 indicates complete disagreement (no shared information). This normalization makes it easier to interpret the degree of dissimilarity between the variables. ::: :::\n\n:::\n:::\n\n## Practical Advice {.small}\n\n-   This normalisation technique does not change the essence of what VI measures but scales its output to a more interpretable range, especially useful when comparing across different pairs of variables or datasets.\n\n-   Remember, this normalised VI still does not indicate directionality or the type of relationship (linear or non-linear) between the variables, unlike correlation coefficients.\n\n## Explaining the Results {.small}\n\n-   This example simulates a non-linear relationship between (x) and (y), calculates the linear correlation to show its limitation in capturing non-linear dependencies, and calculates the Variation of Information, which is not limited to linear relationships and can capture the total amount of shared and unique information between the variables.\n\n-   The Pearson correlation coefficient might be low or not significant because it only measures linear relationships. In contrast, the Variation of Information could be higher, reflecting the complexity of the non-linear relationship that the linear correlation fails to capture.\n\n## Transfer entropy {.small}\n\n-   Financial time series dynamics are at the core of many interesting questions in quantitative finance\n-   In many cases this flow of information between markets can be approximate using these dynamics\n-   As useful extract of the previous concepts in this area is [Transfer entropy](https://en.wikipedia.org/wiki/Transfer_entropy) which measures the amount of directed (time-asymmetric) transfer of information between two random processes.\n-   Transfer entropy is conditional mutual information with the history of the influenced variable as condition.\n-   In standard econometrics, transfer entropy is the non-parametric analogue to granger casuality from a vector autoregressive process.\n-   It generalises to allow for non-linear signal analysis -We will use the `RTransferEntropy` to investigate [@RTE2019]\n\n## Information flows in financial markets {.small}\n\n-   Quantifying information flow between markets and individual stocks is a central tenant of quantitative research\n\n-   CAPM prices stocks in this way, imposing a linear unidirectional relationship between individual stocks and the market return\n\n-   Is this assumption valid?\n\n-   We will test this assumption using a selection of 10 stocks from the S&P 500\n\n## Information flow in financial markets {.small}\n\n-   Due to the need for Monte Carlo bootstrapping the estimation can be run over multicores.\n-   you will need a machine with at least 4 cores to run this in a reasonable time\n-   Using 64 cores the code below ran in 10 minutes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RTransferEntropy)\nlibrary(future)\nlibrary(tidyverse)\n# enable parallel processing\nplan(multisession) # initialised a multicore enviroment\ndata(\"stocks\") # loads data \nstocks %>% glimpse()\nTE<-stocks %>% \n  group_by(ticker) %>% \n  group_split(.keep = TRUE) %>%\n  map(~transfer_entropy(x = .x$ret,y=.x$sp500,shuffles = 500,type = \"bins\",bins=12)) \nnames(TE)<-unique(stocks$ticker)\nsave(TE,file=\"Estimation_cache/Transfer_Entropy_SP.RData\")\n```\n:::\n\n\n## Information flows in financial markets {.small}\n\n-   The results are stored in a list of dataframes, one for each stock\n-   We can now visualise the results\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n-   The results indicate that there is indeed a bi-directional flow of information present.\n-   Notably, there are some significant differences in the flows, for example AMD (Advance Micro Devices) contributes much more information to the market than it receives from it.\n\n## Explanation\n\n-   The stock market index itself is a weighted average of individual stocks. Of course, small stocks have less weight and,thus, may only have limited impact on the index, while large corporations might dominate. Transfer entropy can be used to unveil the information that flows from the individual companies‚Äôreturns to the market. [@RTE2019]\n\n## Conclusion {.small}\n\n-   Correlations are useful at quantifying the linear codependency between random variables.\n\n-   However,when variables X and Y are bound by a nonlinear relationship, the above distance metric misjudges the similarity of these variables.\n-   For non-linear cases, @Lopez2020 argues the normalized mutual information is a more appropriate distance metric.\n    -   It allows us to answer questions regarding the unique information contributed by a random variable, without having to make functional assumptions.\n-   Given that many ML algorithms do not impose a functional form on the data, it makes sense to use them in conjunction with entropy-based features.\n\n## Further reading\n\n-   For a more in-depth discussion on the topic, see [@Lopez2020,@Lopez2018]\n",
    "supporting": [
      "index_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}